{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"User Journey Tracking","text":"<p>This is the technical documentation for DSE's User Journey Tracking project.</p>"},{"location":"setup/","title":"Repository setup","text":"<p>These are instructions for individual contributors to set up the repository locally.</p>"},{"location":"setup/#install-dependencies","title":"Install dependencies","text":"<p>We use <code>uv</code> to manage our Python virtual environments. If you have not yet installed it on your system, you can follow the instructions for it here. Most of the ODI team uses Homebrew to install the package. We do not recommend installing <code>uv</code> using <code>pip</code>: as a tool for managing Python environments, it makes sense for it to live outside of a particular Python distribution.</p>"},{"location":"setup/#2-install-python-dependencies","title":"2. Install Python dependencies","text":"<p>If you prefix your commands with <code>uv run</code> (e.g. <code>uv run dbt build</code>), then <code>uv</code> will automatically make sure that the appropriate dependencies are installed before invoking the command.</p> <p>However, if you want to explicitly ensure that all of the dependencies are installed in the virtual environment, run <pre><code>uv sync\n</code></pre> in the root of the repository.</p> <p>Once the dependencies are installed, you can also \"activate\" the virtual environment (similar to how conda virtual environments are activated) by running <pre><code>source .venv/bin/activate\n</code></pre> from the repository root. With the environment activated, you no longer have to prefix commands with <code>uv run</code>.</p> <p>Which approach to take is largely a matter of personal preference:</p> <ul> <li>Using the <code>uv run</code> prefix is more reliable, as dependencies are always resolved before executing.</li> <li>Using <code>source .venv/bin/activate</code> involves less typing.</li> </ul>"},{"location":"setup/#3-install-dbt-dependencies","title":"3. Install dbt dependencies","text":"<p>dbt comes with some core libraries, but others can be added on. This command installs those extra libraries that are needed for this repo. It will have to be re-run each time the repo starts relying on a new dbt library.</p> <p>To install dbt dependencies, open a terminal and enter:</p> <pre><code>uv run dbt deps --project-dir transform\n</code></pre>"},{"location":"setup/#configure-snowflake","title":"Configure Snowflake","text":"<p>In order to use Snowflake (as well as the terraform validators for the Snowflake configuration) you should set some default local environment variables in your environment. This will depend on your operating system and shell. For Linux and Mac OS systems, as well as users of Windows subsystem for Linux (WSL) it's often set in <code>~/.zshrc</code>, <code>~/.bashrc</code>, or <code>~/.bash_profile</code>.</p> <p>If you use zsh or bash, open your shell configuration file, and add the following lines:</p> <p>Default Transformer role</p> <pre><code># Legacy account identifier\nexport SNOWFLAKE_ACCOUNT=&lt;account-locator&gt;\n# The preferred account identifier is to use name of the account prefixed by its organization (e.g. myorg-account123)\n# Supporting snowflake documentation - https://docs.snowflake.com/en/user-guide/admin-account-identifier\nexport SNOWFLAKE_ACCOUNT=&lt;org_name&gt;-&lt;account_name&gt; # format is organization-account\nexport SNOWFLAKE_USER=&lt;your-username&gt;\nexport SNOWFLAKE_PASSWORD=&lt;your-password&gt;\nexport SNOWFLAKE_ROLE=TRANSFORMER_DEV\nexport SNOWFLAKE_WAREHOUSE=TRANSFORMING_XS_DEV\n</code></pre> <p>This will enable you to perform transforming activities which is needed for dbt. Open a new terminal and verify that the environment variables are set.</p> <p>Switch to Loader role</p> <pre><code># Legacy account identifier\nexport SNOWFLAKE_ACCOUNT=&lt;account-locator&gt;\n# The preferred account identifier is to use name of the account prefixed by its organization (e.g. myorg-account123)\n# Supporting snowflake documentation - https://docs.snowflake.com/en/user-guide/admin-account-identifier\nexport SNOWFLAKE_ACCOUNT=&lt;org_name&gt;-&lt;account_name&gt; # format is organization-account\nexport SNOWFLAKE_USER=&lt;your-username&gt;\nexport SNOWFLAKE_PASSWORD=&lt;your-password&gt;\nexport SNOWFLAKE_ROLE=LOADER_DEV\nexport SNOWFLAKE_WAREHOUSE=LOADING_XS_DEV\n</code></pre> <p>This will enable you to perform loading activities and is needed to which is needed for Airflow or Fivetran. Again, open a new terminal and verify that the environment variables are set.</p>"},{"location":"setup/#configure-dbt","title":"Configure dbt","text":"<p>The connection information for our data warehouses will, in general, live outside of this repository. This is because connection information is both user-specific usually sensitive, so should not be checked into version control. In order to run this project locally, you will need to provide this information in a YAML file located (by default) in <code>~/.dbt/profiles.yml</code> (where <code>~</code> indicates your home directory). To see the absolute path of where dbt is looking for your <code>profiles.yml</code>, run <code>dbt debug</code> and inspect the output.</p> <p>Instructions for writing a <code>profiles.yml</code> are documented here, as well as specific instructions for Snowflake.</p> <p>You can verify that your <code>profiles.yml</code> is configured properly by running</p> <pre><code>dbt debug\n</code></pre> <p>from the dbt project root directory (<code>transform</code>).</p>"},{"location":"setup/#snowflake-project","title":"Snowflake project","text":"<p>A minimal version of a <code>profiles.yml</code> for dbt development with is:</p> <pre><code>dse_snowflake:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: &lt;account-locator&gt;\n      user: &lt;your-username&gt;\n      password: &lt;your-password&gt;\n      authenticator: username_password_mfa\n      role: TRANSFORMER_DEV\n      database: TRANSFORM_DEV\n      warehouse: TRANSFORMING_XS_DEV\n      schema: DBT_&lt;your-name&gt;   # Test schema for development\n      threads: 4\n</code></pre>"},{"location":"setup/#installing-pre-commit-hooks","title":"Installing <code>pre-commit</code> hooks","text":"<p>This project uses pre-commit to lint, format, and generally enforce code quality. These checks are run on every commit, as well as in CI.</p> <p>To set up your pre-commit environment locally run</p> <pre><code>pre-commit install\n</code></pre> <p>The next time you make a commit, the pre-commit hooks will run on the contents of your commit (the first time may be a bit slow as there is some additional setup).</p> <p>You can verify that the pre-commit hooks are working properly by running</p> <p><pre><code>pre-commit run --all-files\n</code></pre> to test every file in the repository against the checks.</p> <p>Some of the checks lint our dbt models, so having the dbt project configured is a requirement to run them, even if you don't intend to use those packages.</p>"},{"location":"writing-documentation/","title":"Writing Documentation","text":"<p>Documentation for this project is built using mkdocs with the material theme and hosted using GitHub Pages. The documentation source files are in the <code>docs/</code> directory and are authored using markdown.</p>"},{"location":"writing-documentation/#local-development","title":"Local Development","text":"<p>To write documentation for this project, make sure that the repository is set up. You should then be able to start a local server for the docs:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then open a web browser to http://localhost:8000 to view the built docs. Any edits you make to the markdown sources should be automatically picked up, and the page should automatically rebuild and refresh.</p>"},{"location":"writing-documentation/#deployment","title":"Deployment","text":"<p>Deployment of the docs for this repository is done automatically upon merging to <code>main</code> using the <code>docs</code> GitHub Action.</p> <p>Built documentation is pushed to the <code>gh-pages</code> branch of the repository, and can be viewed by navigating to the GitHub pages URL for the project.</p>"}]}